{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Plant recognition project\n",
    "\n",
    "## 1 Introduction\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "Accurate identification of plant species is essential for a wide range of use cases starting from biodiversity and conservation project, agriculture project and simple nature explorations among others. So far, this task required a specialist knowledge, was time consuming and was often difficult even for professionals.\n",
    "\n",
    "The image classification algorithms are considered to be as a promising directions in reducing the complexity in plan species classification and assisting the professionals whenever possible. Before the deep learning methods became\n",
    "available, the task was primarily tackled by identifying the leaf shape patterns, however this required a clear picture of a leave against a white background and had limited accuracy. Deep learning methods allow to use “noisy” photo images and provide increased accuracy.\n",
    "\n",
    "A range of academic studies has been conducted in the field [1],[2] There exist a number of mobile apps that aim to tackle the problem: noticeable progress in this way was achieved by several projects and apps like LeafSnap\n",
    "(http://leafsnap.com/), PlantNet (http://identify.plantnet-project.org/) or Folia\n",
    "(http://liris.univ-lyon2.fr/reves/content/en/index.php).\n",
    "\n",
    "In addition, the CLEF evaluation forum (http://www.imageclef.org/) hosted a number of challenges over the last few years aiming to increase the accuracy of the identification, which is the main source of both the dataset and an inspiration for the project in general (http://www.imageclef.org/lifeclef/2015/plant)\n",
    "\n",
    "<img src=\"example.jpg\" alt=\"Example plant picture\" style=\"width: 400px;\"/>\n",
    "\n",
    "---\n",
    "### This notebook\n",
    "\n",
    "In this notebook, I will be documenting my approach to solving the problem and showing the interim results.\n",
    "\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "I have broke down the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 1](#step1): Import Datasets\n",
    "* [Step 2](#step2): Detect Humans\n",
    "* [Step 3](#step3): Detect Dogs\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Set up and Dataset import\n",
    "\n",
    "### Import plant Dataset\n",
    "\n",
    "In the code cell belows, we load the additional libraries for future code, set up global variables and import datasets of plant images. We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and globals\n",
    "\n",
    "In the code cell below, we import all required modules used later on and set up a seed number for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2 \n",
    "import os               \n",
    "import matplotlib.pyplot as plt  \n",
    "import xml.etree.ElementTree as ET                \n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from PIL import ImageFile \n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "random.seed(999) # this is required to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Globals\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "\n",
    "BATCH_SIZE = 32   # tweak to your GPUs capacity\n",
    "IMG_HEIGHT = 224   # ResNetInceptionv2 & Xception like 299, ResNet50 & VGG like 224\n",
    "IMG_WIDTH = IMG_HEIGHT\n",
    "CHANNELS = 3\n",
    "DIMENSIONS = (IMG_HEIGHT,IMG_WIDTH,CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Datasets\n",
    "\n",
    "In the code cell below, we import a dataset of images and corresponding XML files containing metadata, where the file paths are stored in the numpy arrays 'picture_files' and 'metadata_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumes we have the following structure:\n",
    "\n",
    "├── data\n",
    "│   ├── test\n",
    "│   ├── train\n",
    "│   ├── small_data_test\n",
    "│   ├── test_tensor_file.npy\n",
    "│   ├── train_tensor_file.npy\n",
    "├── keras.best.h5\n",
    "├── plant_recognition_project.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define function to load function\n",
    "# in both test and train folders we have a mixture or .jpg and .xml files\n",
    "# we need to treat them separately\n",
    "\n",
    "def load_dataset(path):\n",
    "    path_pictures = path + \"*.jpg\"\n",
    "    path_metadata = path + \"*.xml\"\n",
    "    picture_files = np.array(glob(path_pictures))\n",
    "    metadata_files = np.array(glob(path_metadata))\n",
    "    return picture_files, metadata_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is function to create a dictionary out of an individual xml file\n",
    "\n",
    "def get_xml_metadata(file):\n",
    "    pic_meta = {}\n",
    "    pic_meta['file_name'] = os.path.basename(file)\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    # create a dictionary with all metadata\n",
    "    for child in root:\n",
    "        pic_meta[child.tag] = child.text\n",
    "    return pic_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 91758 total train picture files.\n",
      "There are 91758 total train metadata files. \n",
      "\n",
      "There are 21446 total test picture files.\n",
      "There are 21446 total test metadata files.\n"
     ]
    }
   ],
   "source": [
    "#lets get all data from both the test and the train folders\n",
    "\n",
    "train_path = './data/train/'\n",
    "test_path = './data/test/'\n",
    "\n",
    "train_images, train_metadata = load_dataset(train_path)\n",
    "test_images, test_metadata = load_dataset(test_path)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total train picture files.' % len(train_images))\n",
    "print('There are %d total train metadata files. \\n' % len(train_metadata))\n",
    "\n",
    "print('There are %d total test picture files.' % len(test_images))\n",
    "print('There are %d total test metadata files.' % len(test_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Author': 'liliane roubaudi',\n",
       " 'ClassId': '30052',\n",
       " 'Content': 'Flower',\n",
       " 'Date': '2013-8-13',\n",
       " 'Family': 'Convolvulaceae',\n",
       " 'Genus': 'Convolvulus',\n",
       " 'ImageId2014': '11652',\n",
       " 'Latitude': None,\n",
       " 'LearnTag': 'Train',\n",
       " 'Location': 'Nantua',\n",
       " 'Longitude': None,\n",
       " 'MediaId': '37007',\n",
       " 'ObservationId': '18801',\n",
       " 'ObservationId2014': '16074',\n",
       " 'Species': 'Convolvulus arvensis L.',\n",
       " 'Vote': '4',\n",
       " 'YearInCLEF': 'PlantCLEF2014',\n",
       " 'file_name': '37007.xml'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the xml file structure\n",
    "file_meta = get_xml_metadata(train_metadata[10])\n",
    "file_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So, the plant species is captured in the 'Species' field\n",
    "#lets get all species names for both test and train metadata files\n",
    "\n",
    "test_species = []\n",
    "for file in test_metadata:\n",
    "    metadata_inf = get_xml_metadata(file)\n",
    "    test_species.append(metadata_inf['Species'])\n",
    "\n",
    "train_species = []\n",
    "for file in train_metadata:\n",
    "    metadata_inf = get_xml_metadata(file)\n",
    "    train_species.append(metadata_inf['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# later we will need to have one-hot encoded values of the labels\n",
    "# we will use sklearn to trasfer non-integer list to an integer array\n",
    "# we can do an inverse transfer later as well\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_species)\n",
    "train_to_integer = le.transform(train_species)\n",
    "test_to_integer = le.transform(test_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how we can do one hot encoding \n",
    "Y_train = np_utils.to_categorical(train_to_integer, 1000)\n",
    "Y_test = np_utils.to_categorical(test_to_integer, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "In this project, I will be Keras as a main library to work with CNNs and will be using TensorFlow as backend.\n",
    "The below info is a reminder for me to get the right data in Keras:\n",
    "\n",
    "Keras CNNs require a 4D array (tensor) as input, with shape:\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $ IMG\\_HEIGHT \\times IMG\\_HEIGHT $ pixels. where IMG\\_HEIGHT is defined as a global variable and is dictated by the choise of the CNN. Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, IMG\\_HEIGHT, IMG\\_HEIGHT, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, IMG\\_HEIGHT, IMG\\_HEIGHT, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_HEIGHT))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (IMG_HEIGHT, IMG_HEIGHT, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, IMG_HEIGHT, IMG_HEIGHT, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-process the test data for Keras\n",
    "#but only if it hasn't been already done\n",
    "def test_preproc():\n",
    "    try:\n",
    "        test_tensor = np.load(test_tensor_file.npy)\n",
    "    except:\n",
    "        test_tensors = paths_to_tensor(test_images).astype('float32')/255   \n",
    "        #it does take a fair amount of time to do the pre-processing, \n",
    "        #so I will save the files on the disk to save time should something go wrong\n",
    "        test_tensor_file = '/data/test_tensor_file'\n",
    "        np.save(test_tensor_file,test_tensors,allow_pickle=True)\n",
    "    return test_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pre-process the train data for Keras\n",
    "# the file is so large it actually kills my kernel, so I am splitting the files in 5\n",
    "def train_preproc():\n",
    "    try:\n",
    "        train_tensors_1 = np.load('data/train_tensors_1_f.npy')\n",
    "        print('train_1_loaded')\n",
    "        train_tensors_2 = np.load('data/train_tensors_2_f.npy')\n",
    "        print('train_2_loaded')\n",
    "        train_tensors_3 = np.load('data/train_tensors_3_f.npy')\n",
    "        print('train_3_loaded')\n",
    "        train_tensors_4 = np.load('data/train_tensors_4_f.npy')\n",
    "        print('train_4_loaded')\n",
    "        train_tensors_5 = np.load('data/train_tensors_5_f.npy')\n",
    "        print('train_5_loaded')\n",
    "        \n",
    "        train_tensor = np.vstack([train_tensors_1,train_tensors_2,train_tensors_3,train_tensors_4,train_tensors_5])\n",
    "        \n",
    "        print('concatinated')\n",
    "        \n",
    "        del train_tensors_1\n",
    "        del train_tensors_2\n",
    "        del train_tensors_3\n",
    "        del train_tensors_4\n",
    "        del train_tensors_5\n",
    "        \n",
    "    except:\n",
    "        train_tensors_1 = paths_to_tensor(train_images[:20000]).astype('float32')/255\n",
    "        train_tensors_2 = paths_to_tensor(train_images[20000:40000]).astype('float32')/255\n",
    "        train_tensors_3 = paths_to_tensor(train_images[40000:60000]).astype('float32')/255\n",
    "        train_tensors_4 = paths_to_tensor(train_images[60000:80000]).astype('float32')/255\n",
    "        train_tensors_5 = paths_to_tensor(train_images[80000:]).astype('float32')/255\n",
    "        \n",
    "        train_tensors_1_f = 'data/train_tensors_1_f'\n",
    "        train_tensors_2_f = 'data/train_tensors_2_f'\n",
    "        train_tensors_3_f = 'data/train_tensors_3_f'\n",
    "        train_tensors_4_f = 'data/train_tensors_4_f'\n",
    "        train_tensors_5_f = 'data/train_tensors_5_f'\n",
    "        \n",
    "        np.save(train_tensors_1_f,train_tensors_1,allow_pickle=True)\n",
    "        np.save(train_tensors_2_f,train_tensors_2,allow_pickle=True)\n",
    "        np.save(train_tensors_3_f,train_tensors_3,allow_pickle=True)\n",
    "        np.save(train_tensors_4_f,train_tensors_4,allow_pickle=True)\n",
    "        np.save(train_tensors_5_f,train_tensors_5,allow_pickle=True)\n",
    "    \n",
    "    return train_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#it does take a fair amount of time to do the pre-processing, \n",
    "#so I will save the files on the disk to save time should something go wrong\n",
    "\n",
    "train_tensor= train_preproc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tensors_1 = np.load('data/train_tensors_1_f.npy')\n",
    "print('train_1_loaded')\n",
    "train_tensors_2 = np.load('data/train_tensors_2_f.npy')\n",
    "print('train_2_loaded')\n",
    "train_tensors_3 = np.load('data/train_tensors_3_f.npy')\n",
    "print('train_3_loaded')\n",
    "train_tensors_4 = np.load('data/train_tensors_4_f.npy')\n",
    "print('train_4_loaded')\n",
    "train_tensors_5 = np.load('data/train_tensors_5_f.npy')\n",
    "print('train_5_loaded')\n",
    "        \n",
    "train_tensor = np.vstack([train_tensors_1,train_tensors_2,train_tensors_3,train_tensors_4,train_tensors_5])\n",
    "        \n",
    "print('concatinated')\n",
    "        \n",
    "del train_tensors_1\n",
    "del train_tensors_2\n",
    "del train_tensors_3\n",
    "del train_tensors_4\n",
    "del train_tensors_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tensor_file = 'test_tensor_file'\n",
    "np.save(test_tensor_file,test_tensors,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Initial data exploration\n",
    "\n",
    "In the code cell below, we will take a look at the images we have, take a look at labels and at the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to show a number of images\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred=None, smooth=True):\n",
    "\n",
    "    assert len(images) == len(cls_true)\n",
    "\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(2, 2)\n",
    "\n",
    "    # Adjust vertical spacing.\n",
    "    if cls_pred is None:\n",
    "        hspace = 0.3\n",
    "    else:\n",
    "        hspace = 0.6\n",
    "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
    "\n",
    "    # Interpolation type.\n",
    "    if smooth:\n",
    "        interpolation = 'spline16'\n",
    "    else:\n",
    "        interpolation = 'nearest'\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # There may be less than 4 images, ensure it doesn't crash.\n",
    "        if i < len(images):\n",
    "            \n",
    "            # Load image from file first\n",
    "            img = cv2.imread(images[i])\n",
    "            cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Plot image.\n",
    "            ax.imshow(img, interpolation=interpolation)\n",
    "\n",
    "            # Name of the true class.\n",
    "            cls_true_name = cls_true[i]\n",
    "\n",
    "            # Show true and predicted classes.\n",
    "            if cls_pred is None:\n",
    "                xlabel = \"True: {0}\".format(cls_true_name)\n",
    "            else:\n",
    "                # Name of the predicted class.\n",
    "                cls_pred_name = class_names[cls_pred[i]]\n",
    "\n",
    "                xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, cls_pred_name)\n",
    "\n",
    "            # Show the classes as the label on the x-axis.\n",
    "            ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's see a couple of images\n",
    "\n",
    "images = train_images[0:4]\n",
    "labels = train_species[0:4]\n",
    "plot_images(images=images, cls_true=labels, smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's see how many unique labels we have\n",
    "test_species_unique = np.unique(test_species)\n",
    "train_species_unique = np.unique(train_species)\n",
    "\n",
    "print('There are %d unique species in the train files.' % len(train_species_unique))\n",
    "print('There are %d unique species in the test files.' % len(test_species_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's see if there are any species in the test data not presents in the train dataset\n",
    "np.setdiff1d(test_species_unique,train_species_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, all test species are present in the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how species are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's see how species are distributed\n",
    "import pandas as pd\n",
    "\n",
    "unique_train, counts_train = np.unique(train_species, return_counts=True)\n",
    "unique_test, counts_test = np.unique(test_species, return_counts=True)\n",
    "\n",
    "train_data_species = pd.DataFrame()\n",
    "train_data_species['names'] = unique_train\n",
    "train_data_species['counts'] = counts_train\n",
    "\n",
    "test_data_species = pd.DataFrame()\n",
    "test_data_species['names'] = unique_test\n",
    "test_data_species['counts'] = counts_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_data_species.describe())\n",
    "print(train_data_species.describe())\n",
    "#import seaborn as sns\n",
    "#sns.set(style=\"darkgrid\")\n",
    "#ax = sns.countplot(y=\"names\", data=train_data_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from the summary table we can see some species are more prevalent compared to others in both datasets\n",
    "#there is a large difference between the 25% and 75% in both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect plants\n",
    "\n",
    "In this section, we will use transfer learning based on ResNet50 CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import  metrics, models, regularizers, optimizers\n",
    "from keras.applications import ResNet50 #, Xception, InceptionResNetV2\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained ResNet50 model as a fixed feature extractor, where the last convolutional output of ResNet50 is fed as input to our model. We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "base_model = ResNet50(input_shape=DIMENSIONS, weights='imagenet', include_top=False)\n",
    "\n",
    "# Freeze the layers which you don't want to train. Here I am freezing all of them\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Adding custom Layers \n",
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D(name='avg_pool_2')(x)\n",
    "x = Dense(1000, activation='softmax', name='predictions')(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = base_model.input, output = x)\n",
    "\n",
    "# compile the model \n",
    "model_final.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.Adam(1e-3),\n",
    "    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train and Test generators with Augmentation\n",
    "#the data structure is not arranged in the way to use .flow_from_directory - I will need to use .flow\n",
    "\n",
    "\n",
    "#(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "#y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "#y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "#We rescale the images by dividing every pixel in every image by 255.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = train_datagen.flow(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the train dataset it too large to fit into the memory \n",
    "# so need to create a generator that will return batches of images instead\n",
    "\n",
    "def generate_batches_from_train_folder(images_to_read,labels_to_read, batchsize = 100):\n",
    "    \"\"\"\n",
    "    Generator that returns batches of images ('xs') and labels ('ys') from the train folder\n",
    "    :param string filepath: Full filepath of files to read - this needs to be a list of image files and label_files\n",
    "    :param int batchsize: Size of the batches that should be generated.\n",
    "    :return: (ndarray, ndarray) (xs, ys): Yields a tuple which contains a full batch of images and labels.\n",
    "    \"\"\"\n",
    "    dimensions = (BATCH_SIZE, IMG_HEIGHT, IMG_HEIGHT, 3) # pixels, three channels\n",
    " \n",
    "\n",
    "    # needs to be on a infinite loop for the generator to work\n",
    "    while 1:\n",
    "        filesize = len(images_to_read)\n",
    "\n",
    "        # count how many entries we have read\n",
    "        n_entries = 0\n",
    "        # as long as we haven't read all entries from the file: keep reading\n",
    "        while n_entries < (filesize - batchsize):\n",
    "            \n",
    "            # start the next batch at index 0\n",
    "            # create numpy arrays of input data (features) - this is already shaped as a tensor\n",
    "            xs = paths_to_tensor(images_to_read[n_entries : n_entries + batchsize])\n",
    "\n",
    "            # and label info. Contains more than one label in my case, e.g. is_dog, is_cat, fur_color,...\n",
    "            y_values = f['y'][n_entries:n_entries+batchsize]\n",
    "            ys = np.array(np.zeros((batchsize, 1000))) # data with 2 different classes (e.g. dog or cat)\n",
    "\n",
    "            # Select the labels that we want to use, e.g. is dog/cat\n",
    "            for c, y_val in enumerate(y_values):\n",
    "                ys[c] = encode_targets(y_val, class_type='dog_vs_cat') # returns categorical labels [0,1], [1,0]\n",
    "\n",
    "            # we have read one more batch from this file\n",
    "            n_entries += batchsize\n",
    "            yield (xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"ResNet_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train the model \n",
    "model_out = model_final.fit_generator(\n",
    "train_generator,\n",
    "samples_per_epoch = nb_train_samples,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "nb_val_samples = nb_validation_samples,\n",
    "callbacks = [checkpoint, early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(BEST_MODEL)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=1e-4,),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc'])\n",
    "\n",
    "model_out = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=n_of_train_samples//BATCH_SIZE,\n",
    "    epochs=60,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=n_of_val_samples//BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aind-dog]",
   "language": "python",
   "name": "conda-env-aind-dog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
